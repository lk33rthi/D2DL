{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Manipulation\n",
    "\n",
    "Generally, there are two important things we need to do with data: (i) acquire them; and (ii) process them once they are inside the computer. \n",
    "\n",
    "we use n-dimesnional array which is also called the tensor to store data. \n",
    "a tensor represets array of numerical values\n",
    "with one axis, a tensor corresponds to a vector\n",
    "with two axes, a tensor corresponds to a matrix\n",
    "tensors with more than two axes do not have special name. \n",
    "each values in a tensor is called element of the tensor. \n",
    "\n",
    "Unless otherwise specified a new tensors will be stored in main memory and designated for CPU-based computation. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#arange creates a row vector x containing the first 12 integers starting from 0. \n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#access tensor's shape by inspecting it's shape property\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to know the total number of elements in a tensor \n",
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change the shape of the tensor without altering the number of elements or their values, invoke reshape function\n",
    "X=x.reshape(3,4)\n",
    "X\n",
    "# x.reshape(3, 4) is equivalent to  x.reshape(-1, 4) or x.reshape(3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor representing a tensor with all elements set to 0 and a shape of (2, 3, 4)\n",
    "torch.zeros(2,3,4)\n",
    "#similarly a tensor with ones can be created with torch.ones(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0332,  0.5729,  1.7230,  0.4498],\n",
       "         [-0.3705,  0.1873, -0.0696, -1.3813],\n",
       "         [ 1.7296,  0.5564, -1.5853,  0.3685]],\n",
       "\n",
       "        [[ 0.9239,  0.7719,  0.4398,  0.4306],\n",
       "         [ 0.0485, -1.3356, -0.9727, -0.4116],\n",
       "         [-0.8240, -2.2423,  0.6086,  0.2774]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#elements are randomly sampled from a standard Gaussian distribution with a mean of 0 and standard deviation of 1\n",
    "torch.randn(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [2, 3, 4, 5],\n",
       "        [3, 4, 5, 6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exact values of elements can be specified in desired shape by supplying a python list. \n",
    "torch.tensor([[1,2,3,4],[2,3,4,5],[3,4,5,6]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elementwise operations apply a standard scalar operation to each element of an array. For functions that take two arrays as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements from the two arrays.\n",
    "The common standard arithmetic operators (+, -, *, /, and ** ) have all been lifted to elementwise operations for any identically-shaped tensors of arbitrary shape. We can call elementwise operations on any two tensors of the same shape. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.0000, 5.1000, 5.0000, 8.0000]),\n",
       " tensor([ 2.0000, -0.9000,  1.0000,  2.0000]),\n",
       " tensor([ 8.0000,  6.3000,  6.0000, 15.0000]),\n",
       " tensor([2.0000, 0.7000, 1.5000, 1.6667]),\n",
       " tensor([ 16.0000,   9.2610,   9.0000, 125.0000]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([4, 2.1, 3, 5])\n",
    "y = torch.tensor([2, 3, 2, 3])\n",
    "x + y, x - y, x * y, x / y, x ** y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to elementwise computations, we can also perform linear algebra operations, including vector dot products and matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  3.,  4.,  5.],\n",
       "         [ 1.,  3.,  5.,  7.],\n",
       "         [ 5.,  7.,  9., 11.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  3.,  4.,  5.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  3.,  5.,  7.],\n",
       "         [ 8.,  9., 10., 11.,  5.,  7.,  9., 11.]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate multiple tensors together, stacking them end-to-end to form a larger tensor.\n",
    "#provide a list of tensors and tell the system along which axis to concatenate.\n",
    "x=torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "y=torch.tensor([[2,3.0,4,5],[1,3,5,7],[5,7,9,11]])\n",
    "torch.cat((x,y), dim=0), torch.cat((x,y), dim=1)\n",
    "#dim=0 concatenate two matrices along rows\n",
    "#dim=1 concatenate two matrices along columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False,  True],\n",
       "        [False, False, False,  True]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each position, if X and Y are equal at that position, the corresponding entry in the new tensor takes a value of 1,\n",
    "#otherwise that position takes 0.\n",
    "x==y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summing all the elements in the tensor yields a tensor with only one element.\n",
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when shapes differ, we can still perform elementwise operations by invoking the broadcasting mechanism. \n",
    "This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the elementwise operations on the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1, 2, 3]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.arange(3).reshape(3,1)\n",
    "b=torch.arange(4).reshape(1,4)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a and b are  3×1  and  1×4  matrices respectively, their shapes do not match up if we want to add them. We broadcast the entries of both matrices into a larger  3×4  matrix as follows: for matrix a it replicates the columns and for matrix b it replicates the rows before adding up both elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [2, 3, 4, 5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "elements in a tensor can be accessed by index.the first element has index 0 and ranges are specified to include the first but before the last element. we can access elements according to their relative position to the end of the list by using negative indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[-1] selects the last element\n",
    "x[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[1:3] selects the second and the third elements \n",
    "x[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  5.,  6., 22.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Beyond reading, we can also write elements of a matrix by specifying indices.\n",
    "x[1,3]=22\n",
    "x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to assign multiple elements the same value, we simply index all of them and then assign them the value. For instance, [0:2, :] accesses the first and second rows, where : takes all the elements along axis 1 (column). While we discussed indexing for matrices, this obviously also works for vectors and for tensors of more than 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10,  2,  3],\n",
       "        [10, 10,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(12).reshape(3,4)\n",
    "x\n",
    "x[0:2, 0:2]=10\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Memory\n",
    "if we write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. After running Y = Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then makes Y point to this new location in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before=id(y)\n",
    "y=y+x\n",
    "id(y)==before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform in-place operations, assign the result of an operation to a previously allocated array with slice notation, e.g., Y[:] = <expression>. To illustrate this concept, we first create a new matrix Z with the same shape as another Y, using zeros_like to allocate a block of  0  entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(z):  1993719816664\n",
      "id(z):  1993719816664\n"
     ]
    }
   ],
   "source": [
    "z=torch.zeros_like(y)\n",
    "z\n",
    "print('id(z): ', id(z))\n",
    "z[:]=x+y\n",
    "print('id(z): ', id(z))\n",
    "#X[:] = X + Y is same as X += Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to other python objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To convert a size-1 tensor to a Python scalar, we can invoke the item function or Python’s built-in functions.\n",
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14]]),\n",
       " tensor([[1, 3, 5, 7, 9],\n",
       "         [2, 4, 6, 8, 0],\n",
       "         [2, 3, 4, 5, 6]]),\n",
       " tensor([[ True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False],\n",
       "         [False, False, False, False, False]]),\n",
       " tensor([[False, False, False, False, False],\n",
       "         [ True,  True,  True, False,  True],\n",
       "         [ True,  True,  True,  True,  True]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. Change the conditional statement X == Y in this section to X < Y or X > Y, and then see what kind of tensor you can get.\n",
    "x=torch.arange(15).reshape(3,5)\n",
    "y=torch.tensor([[1,3,5,7,9],[2,4,6,8,0],[2,3,4,5,6]])\n",
    "x, y, x<y, x>y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0, 1, 2]],\n",
       " \n",
       "         [[3, 4, 5]]]),\n",
       " tensor([[[0],\n",
       "          [1]],\n",
       " \n",
       "         [[2],\n",
       "          [3]]]),\n",
       " tensor([[[0, 1, 2],\n",
       "          [1, 2, 3]],\n",
       " \n",
       "         [[5, 6, 7],\n",
       "          [6, 7, 8]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Replace the two tensors that operate by element in the broadcasting mechanism with other shapes, eg., 3-dimensional tensors. \n",
    "#Is the result the same as expected?\n",
    "a=torch.arange(6).reshape(2,1,3)\n",
    "b=torch.arange(4).reshape(2,2,1)\n",
    "a,b, a+b\n",
    "\n",
    "#while adding two 3-d tensors of different shapes, either each dimension of one tensor should be one or same as the other tensor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Preprocessing\n",
    "preprocessing raw data with pandas and converting them into the tensor format. \n",
    "\n",
    "Reading the data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Position</th>\n",
       "      <th>Year</th>\n",
       "      <th>Team</th>\n",
       "      <th>Games Played</th>\n",
       "      <th>Passes Attempted</th>\n",
       "      <th>Passes Completed</th>\n",
       "      <th>Completion Percentage</th>\n",
       "      <th>Pass Attempts Per Game</th>\n",
       "      <th>...</th>\n",
       "      <th>TD Passes</th>\n",
       "      <th>Percentage of TDs per Attempts</th>\n",
       "      <th>Ints</th>\n",
       "      <th>Int Rate</th>\n",
       "      <th>Longest Pass</th>\n",
       "      <th>Passes Longer than 20 Yards</th>\n",
       "      <th>Passes Longer than 40 Yards</th>\n",
       "      <th>Sacks</th>\n",
       "      <th>Sacked Yards Lost</th>\n",
       "      <th>Passer Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Player Id, Name, Position, Year, Team, Games Played, Passes Attempted, Passes Completed, Completion Percentage, Pass Attempts Per Game, Passing Yards, Passing Yards Per Attempt, Passing Yards Per Game, TD Passes, Percentage of TDs per Attempts, Ints, Int Rate, Longest Pass, Passes Longer than 20 Yards, Passes Longer than 40 Yards, Sacks, Sacked Yards Lost, Passer Rating]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 23 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data =pd.read_csv(\"Career_Stats_Passing.csv\")\n",
    "data.head(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data. \"NaN\" entries are missing values. To handle missing data, typical methods include imputation and deletion, where imputation replaces missing values with substituted ones, while deletion ignores missing values. \n",
    "\n",
    "Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player Id</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Team</th>\n",
       "      <th>Games Played</th>\n",
       "      <th>Passes Attempted</th>\n",
       "      <th>Passes Completed</th>\n",
       "      <th>Completion Percentage</th>\n",
       "      <th>Pass Attempts Per Game</th>\n",
       "      <th>Passing Yards</th>\n",
       "      <th>...</th>\n",
       "      <th>TD Passes</th>\n",
       "      <th>Percentage of TDs per Attempts</th>\n",
       "      <th>Ints</th>\n",
       "      <th>Int Rate</th>\n",
       "      <th>Longest Pass</th>\n",
       "      <th>Passes Longer than 20 Yards</th>\n",
       "      <th>Passes Longer than 40 Yards</th>\n",
       "      <th>Sacks</th>\n",
       "      <th>Sacked Yards Lost</th>\n",
       "      <th>Passer Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Player Id, Name, Year, Team, Games Played, Passes Attempted, Passes Completed, Completion Percentage, Pass Attempts Per Game, Passing Yards, Passing Yards Per Attempt, Passing Yards Per Game, TD Passes, Percentage of TDs per Attempts, Ints, Int Rate, Longest Pass, Passes Longer than 20 Yards, Passes Longer than 40 Yards, Sacks, Sacked Yards Lost, Passer Rating]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Delete the column with the most missing values.\n",
    "\n",
    "a=[list(dict(data.isna().sum()).values()).index(max(data.isna().sum()))]\n",
    "a\n",
    "data.drop(data.columns[a], axis=1, inplace=True)\n",
    "data.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the preprocessed dataset to the tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Linear Algebra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19],\n",
       "         [20, 21, 22, 23, 24]]),\n",
       " tensor([[ 0,  5, 10, 15, 20],\n",
       "         [ 1,  6, 11, 16, 21],\n",
       "         [ 2,  7, 12, 17, 22],\n",
       "         [ 3,  8, 13, 18, 23],\n",
       "         [ 4,  9, 14, 19, 24]]),\n",
       " tensor([[ True, False, False, False, False],\n",
       "         [False,  True, False, False, False],\n",
       "         [False, False,  True, False, False],\n",
       "         [False, False, False,  True, False],\n",
       "         [False, False, False, False,  True]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Matrices\n",
    "A=torch.arange(25).reshape(5,5)\n",
    "A, A.T, A==A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors\n",
    "given any two tensors with the same shape, the result of any binary elementwise operation will be a tensor of that same shape. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 3,  4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11, 12]],\n",
       " \n",
       "         [[13, 14, 15, 16, 17],\n",
       "          [18, 19, 20, 21, 22]]]),\n",
       " torch.Size([2, 2, 5]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "t1=torch.arange(20).reshape(2,2,5)\n",
    "t1+x, (t1*x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor(66.))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reduction\n",
    "#One useful operation that we can perform with arbitrary tensors is to calculate the sum of their elements.\n",
    "m2=torch.arange(12, dtype=torch.float32).reshape(3,4)\n",
    "m2, m2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([12., 15., 18., 21.]), torch.Size([4]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To reduce the row dimension (axis 0) by summing up elements of all the rows, we specify axis=0 when invoking the function. \n",
    "m2_sum_axis0=m2.sum(axis=0)\n",
    "m2_sum_axis0, m2_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 6., 22., 38.]), torch.Size([3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specifying axis=1 will reduce the column dimension (axis 1) by summing up elements of all the columns.\n",
    "m2_sum_axis1=m2.sum(axis=1)\n",
    "m2_sum_axis1, m2_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(66.), torch.Size([]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reducing a matrix along both rows and columns via summation is equivalent to summing up all the elements of the matrix.\n",
    "m2_sum=m2.sum(axis=[0,1])\n",
    "m2_sum, m2_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4., 5., 6., 7.]), tensor([4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the function for calculating the mean can also reduce a tensor along the specified axes\n",
    "m2_mean_axis0=m2.mean(axis=0)\n",
    "m2_mean_axis0, m2.sum(axis=0)/m2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5000, 5.5000, 9.5000]), tensor([1.5000, 5.5000, 9.5000]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_mean_axis1=m2.mean(axis=1)\n",
    "m2_mean_axis1, m2.sum(axis=1)/m2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[12., 15., 18., 21.]]),\n",
       " torch.Size([1, 4]),\n",
       " tensor([[ 6.],\n",
       "         [22.],\n",
       "         [38.]]),\n",
       " torch.Size([3, 1]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#non-reduction sum\n",
    "#sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean.\n",
    "m2_sum_axis0=m2.sum(axis=0, keepdims=True)\n",
    "m2_sum_axis1=m2.sum(axis=1, keepdims=True)\n",
    "m2, m2_sum_axis0, m2_sum_axis0.shape, m2_sum_axis1, m2_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([0., 1., 2., 3.]), tensor(14.), tensor(14.))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dot product\n",
    "#Given two vectors  x,y, their dot product is a sum over the products of the elements at the same position.\n",
    "p=torch.arange(4, dtype=torch.float32)\n",
    "q=torch.arange(4, dtype=torch.float32)\n",
    "p, q, torch.dot(p,q), torch.sum(p*q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " torch.Size([3, 4]),\n",
       " tensor([0., 1., 2., 3.]),\n",
       " torch.Size([4]),\n",
       " tensor([14., 38., 62.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matrix-vector products\n",
    "#When we call np.dot(A, x) with a matrix A and a vector x, the matrix-vector product is performed.\n",
    "#the column dimension of A (its length along axis 1) must be the same as the dimension of x (its length)\n",
    "m2, m2.shape, p, p.shape, torch.mv(m2, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3]),\n",
       " torch.Size([3, 4]),\n",
       " tensor([[ 20.,  23.,  26.,  29.],\n",
       "         [ 56.,  68.,  80.,  92.],\n",
       "         [ 92., 113., 134., 155.],\n",
       "         [128., 158., 188., 218.]]),\n",
       " torch.Size([4, 4]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#matrix-matrix multiplication\n",
    "m1=torch.clone(m2)\n",
    "m1=m1.reshape(4,3)\n",
    "m1.shape, m2.shape, torch.mm(m1,m2), torch.mm(m1,m2).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.), tensor(7.))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#norms\n",
    "#the norm of a vector tells us how big a vector is.\n",
    "#The  L2  norm of vector x  is the square root of the sum of the squares of the vector elements.\n",
    "#L1  norm, which is expressed as the sum of the absolute values of the vector elements\n",
    "u=torch.tensor([3.0, -4.0])\n",
    "torch.norm(u), torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Both the  L2  norm and the  L1  norm are special cases of the more general  Lp  norm\n",
    "# is the square root of the sum of the squares of the matrix elements\n",
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]),\n",
       " torch.Size([4, 3]),\n",
       " torch.Size([4, 3]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Prove that the transpose of a matrix  A’s transpose is  A.\n",
    "#d1=Transpose of m1, d2=transpose of transpose of m1\n",
    "d1=m1.T\n",
    "d2=d1.T\n",
    "m1==d2, m1.shape, d2.shape\n",
    "#Hence proved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Given two matrices  A  and  C , show that the sum of transposes is equal to the transpose of a sum.\n",
    "A.shape\n",
    "C=torch.ones(5,5)\n",
    "AT=A.T\n",
    "CT=C.T\n",
    "AT+CT==(A+C).T\n",
    "#Hence Proved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Given any square matrix  A, sum of A and A transpose is always symmetric. Why?\n",
    "#Symmetric matrix is a square matrix that is equal to its transpose\n",
    "SY=A+AT\n",
    "SY==SY.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. We defined the tensor Z of shape (2, 3, 4) in this section. What is the output of len(Z)?\n",
    "# The length of a vector is commonly called the dimension of the vector.\n",
    "Z=torch.arange(24).reshape(2,3,4)\n",
    "len(Z)\n",
    "#len(Z) is 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
